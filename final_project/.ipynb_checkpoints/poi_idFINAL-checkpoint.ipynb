{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "########################################### Task 1: Feature Selection\n",
    "\n",
    "# Removed 'email_address' (string) to prepare for ML algorithm\n",
    "# Removed 'Other' as this this quite ambiguous in nature\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances',\n",
    "                 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value',\n",
    "                 'expenses', 'exercised_stock_options', 'long_term_incentive',\n",
    "                 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person',\n",
    "                 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_nan_values():\n",
    "    # Update NaN values with 0 except for email address\n",
    "    people_keys = data_dict.keys()\n",
    "    feature_keys = data_dict[people_keys[0]]\n",
    "    nan_features = {}\n",
    "    # Get list of NaN values and replace them\n",
    "    for feature in feature_keys:\n",
    "        nan_features[feature] = 0\n",
    "    for person in people_keys:\n",
    "        for feature in feature_keys:\n",
    "            if feature != 'email_address' and \\\n",
    "                data_dict[person][feature] == 'NaN':\n",
    "                data_dict[person][feature] = 0\n",
    "                nan_features[feature] += 1\n",
    "\n",
    "    return nan_features\n",
    "\n",
    "\n",
    "def poi_missing_email_info():\n",
    "    # Find total count and values of POI with missing or no to/from email information\n",
    "    poi_count = 0\n",
    "    poi_keys = []\n",
    "    for person in data_dict.keys():\n",
    "        if data_dict[person][\"poi\"]:\n",
    "            poi_count += 1\n",
    "            poi_keys.append(person)\n",
    "\n",
    "    poi_missing_emails = []\n",
    "    for poi in poi_keys:\n",
    "        if (data_dict[poi]['to_messages'] == 'NaN' and data_dict[poi]['from_messages'] == 'NaN') or \\\n",
    "            (data_dict[poi]['to_messages'] == 0 and data_dict[poi]['from_messages'] == 0):\n",
    "            poi_missing_emails.append(poi)\n",
    "\n",
    "    return poi_count, poi_missing_emails\n",
    "\n",
    "\n",
    "def salary_bonus_crazy():\n",
    "    # Identify salary or bonus outliers\n",
    "    crazy = []\n",
    "    people_keys = data_dict.keys()\n",
    "    for person in people_keys:\n",
    "        if data_dict[person][\"bonus\"] > 5000000 or data_dict[person][\"salary\"] > 1000000:\n",
    "            crazy.append(person)\n",
    "\n",
    "    return crazy\n",
    "\n",
    "\n",
    "def create_new_features():\n",
    "    # Create new features for possible use in feature selection\n",
    "    people_keys = data_dict.keys()\n",
    "\n",
    "    for person in people_keys:\n",
    "        to_poi = float(data_dict[person]['from_this_person_to_poi'])\n",
    "        from_poi = float(data_dict[person]['from_poi_to_this_person'])\n",
    "        to_msg_total = float(data_dict[person]['to_messages'])\n",
    "        from_msg_total = float(data_dict[person]['from_messages'])\n",
    "\n",
    "        if from_msg_total > 0:\n",
    "            data_dict[person]['to_poi_fraction'] = to_poi / from_msg_total\n",
    "        else:\n",
    "            data_dict[person]['to_poi_fraction'] = 0\n",
    "\n",
    "        if to_msg_total > 0:\n",
    "            data_dict[person]['from_poi_fraction'] = from_poi / to_msg_total\n",
    "        else:\n",
    "            data_dict[person]['from_poi_fraction'] = 0\n",
    "\n",
    "        # fraction of your salary represented by your bonus (or something like that)\n",
    "        person_salary = float(data_dict[person]['salary'])\n",
    "        person_bonus = float(data_dict[person]['bonus'])\n",
    "        if person_salary > 0 and person_bonus > 0:\n",
    "            data_dict[person]['salary_bonus_fraction'] = data_dict[person]['salary'] / data_dict[person]['bonus']\n",
    "        else:\n",
    "            data_dict[person]['salary_bonus_fraction'] = 0\n",
    "\n",
    "    # Add new feature to features_list\n",
    "    features_list.extend(['to_poi_fraction', 'from_poi_fraction', 'salary_bonus_fraction'])\n",
    "\n",
    "\n",
    "def explore_data():\n",
    "    # main data exploration function\n",
    "    people_keys = data_dict.keys()\n",
    "    feature_keys = data_dict[people_keys[0]]\n",
    "    poi_cnt, poi_missing_emails = poi_missing_email_info()\n",
    "\n",
    "    print 'Number of people in dataset: %d' % len(people_keys)\n",
    "    print 'Number of features for each person: %d' % len(feature_keys)\n",
    "    print 'Number of Persons of Interests (POIs) in dataset: %d out of 34 total POIs' % poi_cnt\n",
    "    print 'Number of non-POIs in dataset: %d' % (len(people_keys) - poi_cnt)\n",
    "    print 'POIs with zero or missing to/from email messages in dataset: %d' % len(poi_missing_emails)\n",
    "    print poi_missing_emails\n",
    "\n",
    "    print '\\n'\n",
    "    print 'Removing Outliers'\n",
    "    print '\\n'\n",
    "    # Update nan values in features, not good for numeric comparisons like > < ==\n",
    "    features_with_nan = fill_nan_values()\n",
    "    print 'Updating NaN values in features'\n",
    "    print features_with_nan\n",
    "\n",
    "    # Outlier at 26M in salary -> 'total \n",
    "    #Remove outlier 'total'\n",
    "    print 'Outlier is \"Total\" value and should be removed'\n",
    "\n",
    "    # Investigate other high salary or bonuses for outliers\n",
    "    high_salary_bonus = salary_bonus_crazy()\n",
    "    print 'Salary Bonus Crazy (1M+ and 5M+): \\n', high_salary_bonus\n",
    "\n",
    "    # Only 146 values, can visually review names\n",
    "    print 'Look for other \"odd\" values to remove'\n",
    "    print '\\n'\n",
    "    print people_keys\n",
    "    print '\\n'\n",
    "    print 'Found name: \"THE TRAVEL AGENCY IN THE PARK\" '\n",
    "\n",
    "    # Remove outlier and odd person value\n",
    "    print '\\n'\n",
    "    print 'Removing two values: Total, The Travel Agency In The Park'\n",
    "    data_dict.pop('TOTAL')\n",
    "    data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "    \n",
    "\n",
    "    # Create new features\n",
    "    print '\\n'\n",
    "    print 'Create Features'\n",
    "    print '\\n'\n",
    "    create_new_features()\n",
    "    print 'Updated features_list: \\n', features_list\n",
    "\n",
    "\n",
    "def build_classifier_pipeline(classifier_type, kbest, f_list):\n",
    "    # Build pipeline and tune parameters via GridSearchCV\n",
    "\n",
    "    data = featureFormat(my_dataset, f_list, sort_keys=True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "\n",
    "    # Using stratified shuffle split cross validation because of the small size of the dataset\n",
    "    sss = StratifiedShuffleSplit(labels, 500, test_size=0.45, random_state=42)\n",
    "\n",
    "    # Build pipeline\n",
    "    kbest = SelectKBest(k=kbest)\n",
    "    scaler = MinMaxScaler()\n",
    "    classifier = set_classifier(classifier_type)\n",
    "    pipeline = Pipeline(steps=[('minmax_scaler', scaler), ('feature_selection', kbest), (classifier_type, classifier)])\n",
    "\n",
    "    # Set parameters for random forest\n",
    "    parameters = []\n",
    "    if classifier_type == 'random_forest':\n",
    "        parameters = dict(random_forest__n_estimators=[25, 50],\n",
    "                          random_forest__min_samples_split=[2, 3, 4],\n",
    "                          random_forest__criterion=['gini', 'entropy'])\n",
    "    if classifier_type == 'logistic_reg':\n",
    "        parameters = dict(logistic_reg__class_weight=['balanced'],\n",
    "                          logistic_reg__solver=['liblinear'],\n",
    "                          logistic_reg__C=range(1, 5),\n",
    "                          logistic_reg__random_state=42)\n",
    "    if classifier_type == 'decision_tree':\n",
    "        parameters = dict(decision_tree__min_samples_leaf=range(1, 5),\n",
    "                          decision_tree__mix_depth=range(1, 5),\n",
    "                          decision_tree__class_weight=['balanced'],\n",
    "                          decision_tree__criterion=['gini', 'entropy'])\n",
    "\n",
    "    # Get optimized parameters for F1-scoring metrics\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters, scoring='f1', cv=sss)\n",
    "    t0 = time()\n",
    "    cv.fit(features, labels)\n",
    "    print 'Classifier tuning: %r' % round(time() - t0, 3)\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "def set_classifier(x):\n",
    "    # switch statement Python replacement - http://stackoverflow.com/a/103081\n",
    "    return {\n",
    "        'random_forest': RandomForestClassifier(),\n",
    "        'decision_tree': DecisionTreeClassifier(),\n",
    "        'logistic_reg': LogisticRegression(),\n",
    "        'gaussian_nb': GaussianNB()\n",
    "    }.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Load Dataset:\n",
      "\n",
      "\n",
      "data_dict of length 146 loaded successfully\n",
      "\n",
      "\n",
      "Data Exploration:\n",
      "\n",
      "\n",
      "Number of people in dataset: 146\n",
      "Number of features for each person: 21\n",
      "Number of Persons of Interests (POIs) in dataset: 18 out of 34 total POIs\n",
      "Number of non-POIs in dataset: 128\n",
      "POIs with zero or missing to/from email messages in dataset: 4\n",
      "['KOPPER MICHAEL J', 'FASTOW ANDREW S', 'YEAGER F SCOTT', 'HIRKO JOSEPH']\n",
      "\n",
      "\n",
      "Removing Outliers\n",
      "\n",
      "\n",
      "Updating NaN values in features\n",
      "{'salary': 51, 'to_messages': 60, 'deferral_payments': 107, 'total_payments': 21, 'loan_advances': 142, 'bonus': 64, 'email_address': 0, 'restricted_stock_deferred': 128, 'total_stock_value': 20, 'shared_receipt_with_poi': 60, 'long_term_incentive': 80, 'exercised_stock_options': 44, 'from_messages': 60, 'other': 53, 'from_poi_to_this_person': 60, 'from_this_person_to_poi': 60, 'poi': 0, 'deferred_income': 97, 'expenses': 51, 'restricted_stock': 36, 'director_fees': 129}\n",
      "Outlier is \"Total\" value and should be removed\n",
      "Salary Bonus Crazy (1M+ and 5M+): \n",
      "['LAVORATO JOHN J', 'LAY KENNETH L', 'BELDEN TIMOTHY N', 'SKILLING JEFFREY K', 'TOTAL', 'FREVERT MARK A']\n",
      "Look for other \"odd\" values to remove\n",
      "\n",
      "\n",
      "['METTS MARK', 'BAXTER JOHN C', 'ELLIOTT STEVEN', 'CORDES WILLIAM R', 'HANNON KEVIN P', 'MORDAUNT KRISTINA M', 'MEYER ROCKFORD G', 'MCMAHON JEFFREY', 'HORTON STANLEY C', 'PIPER GREGORY F', 'HUMPHREY GENE E', 'UMANOFF ADAM S', 'BLACHMAN JEREMY M', 'SUNDE MARTIN', 'GIBBS DANA R', 'LOWRY CHARLES P', 'COLWELL WESLEY', 'MULLER MARK S', 'JACKSON CHARLENE R', 'WESTFAHL RICHARD K', 'WALTERS GARETH W', 'WALLS JR ROBERT H', 'KITCHEN LOUISE', 'CHAN RONNIE', 'BELFER ROBERT', 'SHANKMAN JEFFREY A', 'WODRASKA JOHN', 'BERGSIEKER RICHARD P', 'URQUHART JOHN A', 'BIBI PHILIPPE A', 'RIEKER PAULA H', 'WHALEY DAVID A', 'BECK SALLY W', 'HAUG DAVID L', 'ECHOLS JOHN B', 'MENDELSOHN JOHN', 'HICKERSON GARY J', 'CLINE KENNETH W', 'LEWIS RICHARD', 'HAYES ROBERT E', 'MCCARTY DANNY J', 'KOPPER MICHAEL J', 'LEFF DANIEL P', 'LAVORATO JOHN J', 'BERBERIAN DAVID', 'DETMERING TIMOTHY J', 'WAKEHAM JOHN', 'POWERS WILLIAM', 'GOLD JOSEPH', 'BANNANTINE JAMES M', 'DUNCAN JOHN H', 'SHAPIRO RICHARD S', 'SHERRIFF JOHN R', 'SHELBY REX', 'LEMAISTRE CHARLES', 'DEFFNER JOSEPH M', 'KISHKILL JOSEPH G', 'WHALLEY LAWRENCE G', 'MCCONNELL MICHAEL S', 'PIRO JIM', 'DELAINEY DAVID W', 'SULLIVAN-SHAKLOVITZ COLLEEN', 'WROBEL BRUCE', 'LINDHOLM TOD A', 'MEYER JEROME J', 'LAY KENNETH L', 'BUTTS ROBERT H', 'OLSON CINDY K', 'MCDONALD REBECCA', 'CUMBERLAND MICHAEL S', 'GAHN ROBERT S', 'MCCLELLAN GEORGE', 'HERMANN ROBERT J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'HAEDICKE MARK E', 'BOWEN JR RAYMOND M', 'GILLIS JOHN', 'FITZGERALD JAY L', 'MORAN MICHAEL P', 'REDMOND BRIAN L', 'BAZELIDES PHILIP J', 'BELDEN TIMOTHY N', 'DURAN WILLIAM D', 'THORN TERENCE H', 'FASTOW ANDREW S', 'FOY JOE', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'KAMINSKI WINCENTY J', 'LOCKHART EUGENE E', 'COX DAVID', 'OVERDYKE JR JERE C', 'PEREIRA PAULO V. FERRAZ', 'STABLER FRANK', 'SKILLING JEFFREY K', 'BLAKE JR. NORMAN P', 'SHERRICK JEFFREY B', 'PRENTICE JAMES', 'GRAY RODNEY', 'PICKERING MARK R', 'THE TRAVEL AGENCY IN THE PARK', 'NOLES JAMES L', 'KEAN STEVEN J', 'TOTAL', 'FOWLER PEGGY', 'WASAFF GEORGE', 'WHITE JR THOMAS E', 'CHRISTODOULOU DIOMEDES', 'ALLEN PHILLIP K', 'SHARP VICTORIA T', 'JAEDICKE ROBERT', 'WINOKUR JR. HERBERT S', 'BROWN MICHAEL', 'BADUM JAMES P', 'HUGHES JAMES A', 'REYNOLDS LAWRENCE', 'DIMICHELE RICHARD G', 'BHATNAGAR SANJAY', 'CARTER REBECCA C', 'BUCHANAN HAROLD G', 'YEAP SOON', 'MURRAY JULIA H', 'GARLAND C KEVIN', 'DODSON KEITH', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'DIETRICH JANET R', 'DERRICK JR. JAMES V', 'FREVERT MARK A', 'PAI LOU L', 'BAY FRANKLIN R', 'HAYSLETT RODERICK J', 'FUGH JOHN L', 'FALLON JAMES B', 'KOENIG MARK E', 'SAVAGE FRANK', 'IZZO LAWRENCE L', 'TILNEY ELIZABETH A', 'MARTIN AMANDA K', 'BUY RICHARD B', 'GRAMM WENDY L', 'CAUSEY RICHARD A', 'TAYLOR MITCHELL S', 'DONAHUE JR JEFFREY M', 'GLISAN JR BEN F']\n",
      "\n",
      "\n",
      "Found name: \"THE TRAVEL AGENCY IN THE PARK\" \n",
      "\n",
      "\n",
      "Removing two values: Total, The Travel Agency In The Park\n",
      "\n",
      "\n",
      "Create Features\n",
      "\n",
      "\n",
      "Updated features_list: \n",
      "['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_poi_fraction', 'from_poi_fraction', 'salary_bonus_fraction']\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary containing the dataset\n",
    "print '\\n'\n",
    "print 'Load Dataset:'\n",
    "print '\\n'\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    print 'data_dict of length %d loaded successfully' % len(data_dict)\n",
    "\n",
    "\n",
    "# Data Exploration and removal of outliers\n",
    "print '\\n'\n",
    "print 'Data Exploration:'\n",
    "print '\\n'\n",
    "explore_data()\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Feature Selection:\n",
      "\n",
      "\n",
      "[  3.03359535e+00   1.10326985e+01   2.86145313e+00   9.54645949e-01\n",
      "   1.12215484e+01   7.33888265e+00   3.63794367e+00   6.05534301e-04\n",
      "   1.08371937e-01   1.32564377e-01]\n",
      "features: salary score: 3.033595\n",
      "features: total_stock_value score: 11.032698\n",
      "features: expenses score: 2.861453\n",
      "features: bonus score: 0.954646\n",
      "features: exercised_stock_options score: 11.221548\n",
      "features: deferred_income score: 7.338883\n",
      "features: to_poi_fraction score: 3.637944\n",
      "features: from_poi_to_this_person score: 0.000606\n",
      "features: from_poi_fraction score: 0.108372\n",
      "features: shared_receipt_with_poi score: 0.132564\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "print '\\n'\n",
    "print 'Feature Selection:'\n",
    "print '\\n'\n",
    "# Feature select is performed with SelectKBest where k is selected by GridSearchCV\n",
    "# Using Stratify for small and minority POI dataset\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "train_test_split(features, labels, train_size=.45, stratify=labels)\n",
    "\n",
    "skbest = SelectKBest(k=10)  # try best value to fit\n",
    "sk_transform = skbest.fit_transform(features_train, labels_train)\n",
    "indices = skbest.get_support(True)\n",
    "print skbest.scores_\n",
    "\n",
    "n_list = ['poi']\n",
    "for index in indices:\n",
    "    print 'features: %s score: %f' % (features_list[index + 1], skbest.scores_[index])\n",
    "n_list.append(features_list[index + 1])\n",
    "\n",
    "\n",
    "# Final features list determined from SelectKBest and manual selection\n",
    "n_list = ['poi', 'salary', 'total_stock_value', 'expenses', 'bonus',\n",
    "          'exercised_stock_options', 'deferred_income',\n",
    "          'to_poi_fraction', 'from_poi_to_this_person', 'from_poi_fraction',\n",
    "          'shared_receipt_with_poi']\n",
    "\n",
    "# Update features_list with new values\n",
    "features_list = n_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test and Tune Classifiers:\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=0.8, n_estimators=50, random_state=None)\n",
      "\tAccuracy: 0.84927\tPrecision: 0.43070\tRecall: 0.40550\tF1: 0.41772\tF2: 0.41030\n",
      "\tTotal predictions: 15000\tTrue positives:  811\tFalse positives: 1072\tFalse negatives: 1189\tTrue negatives: 11928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test classifiers\n",
    "print '\\n'\n",
    "print 'Test and Tune Classifiers:\\n'\n",
    "# Tune your classifier to achieve better than .3 precision and recall using our testing script.\n",
    "# See \"build_classifier_pipeline\" for MinMaxScaling, SelectKBest and Logistic Regression tuning\n",
    "\n",
    "# Classifiers tested but not using - Logistic_Regression, RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "#cross_val = build_classifier_pipeline('logistic_reg', 10, features_list)\n",
    "#print 'Best parameters: ', cross_val.best_params_\n",
    "#clf = cross_val.best_estimator_\n",
    "\n",
    "\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, min_samples_leaf=2, class_weight='balanced'),\n",
    "                         n_estimators=50, learning_rate=.8)\n",
    "\n",
    "\n",
    "# Validate model precision, recall and F1-score\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dump Classifiers, dataset and features_list\n",
      "\n",
      "\n",
      "Successfully created clf, my_dataset and features_list pkl files\n",
      "\n",
      "\n",
      "References Used\n",
      "Udacity Data Analyst Course: \n",
      "https://www.udacity.com/course/data-analyst-nanodegree--nd002?v=a4/ \n",
      "Scikit-learn Documentation: \n",
      "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html \n",
      "Scikit-learn Documentation: \n",
      "http://scikit-learn.org/stable/modules/pipeline.html \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump classifier, dataset and features_list\n",
    "print '\\n'\n",
    "print 'Dump Classifiers, dataset and features_list'\n",
    "print '\\n'\n",
    "# Dump your classifier, dataset, and features_list so anyone can\n",
    "# check your results. You do not need to change anything below, but make sure\n",
    "# that the version of poi_id.py that you submit can be run on its own and\n",
    "# generates the necessary .pkl files for validating your results.\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print 'Successfully created clf, my_dataset and features_list pkl files'\n",
    "\n",
    "# References\n",
    "print '\\n'\n",
    "print 'References Used'\n",
    "print 'Udacity Data Analyst Course: \\nhttps://www.udacity.com/course/data-analyst-nanodegree--nd002?v=a4/ \\n' \\\n",
    "        'Scikit-learn Documentation: \\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html \\n' \\\n",
    "        'Scikit-learn Documentation: \\nhttp://scikit-learn.org/stable/modules/pipeline.html \\n' \\"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
